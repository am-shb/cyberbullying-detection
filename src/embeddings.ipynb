{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Mar 26 2022\n",
        "@author: Amirmohammad Shahbandegan\n",
        "\"\"\"\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTdNNgJzIBtQ",
        "outputId": "17b27e27-f5d2-4fa4-80d0-9e6d3fa36580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 33.1 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 36.9 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 33.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 33.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: emoji, sentence-transformers\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=dccff71f524d6ebb25a28b0e83d14b599d4867a43371640c516eef9259e9d18f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=01f6cc1e87e5770d5ebfe11b90f6874a93049837d4f45c9743c2d3df491c7533\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built emoji sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, pyahocorasick, huggingface-hub, anyascii, transformers, textsearch, sentencepiece, sentence-transformers, emoji, contractions\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.68 emoji-1.7.0 huggingface-hub-0.4.0 pyahocorasick-1.4.4 pyyaml-6.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 textsearch-0.0.21 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji nltk spacy contractions sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ohmp7mqUIxTU",
        "outputId": "3dfb1996-d9d5-478b-a7d0-520820a96dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi3eb_M3aP39",
        "outputId": "bf288fbe-bf01-4873-ca3a-6beb1312690e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-04-01 14:41:27--  https://github.com/am-shb/cyberbullying-detection/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/am-shb/cyberbullying-detection/zip/refs/heads/main [following]\n",
            "--2022-04-01 14:41:27--  https://codeload.github.com/am-shb/cyberbullying-detection/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘main.zip’\n",
            "\n",
            "main.zip                [ <=>                ]   2.79M  14.6MB/s    in 0.2s    \n",
            "\n",
            "2022-04-01 14:41:28 (14.6 MB/s) - ‘main.zip’ saved [2921649]\n",
            "\n",
            "Archive:  main.zip\n",
            "fe5462c9435bb19ee323b45f631ca89e77f39b9e\n",
            "   creating: cyberbullying-detection-main/\n",
            " extracting: cyberbullying-detection-main/.gitignore  \n",
            " extracting: cyberbullying-detection-main/README.md  \n",
            "   creating: cyberbullying-detection-main/data/\n",
            "  inflating: cyberbullying-detection-main/data/cyberbullying_tweets.csv  \n",
            "   creating: cyberbullying-detection-main/src/\n",
            "   creating: cyberbullying-detection-main/src/embeddings/\n",
            "  inflating: cyberbullying-detection-main/src/embeddings/BaseTextEmbeddingProvider.py  \n",
            "  inflating: cyberbullying-detection-main/src/embeddings/FrequencyVectorizer.py  \n",
            "  inflating: cyberbullying-detection-main/src/embeddings/SbertVectorizer.py  \n",
            "  inflating: cyberbullying-detection-main/src/embeddings/Word2vecVectorizer.py  \n",
            "  inflating: cyberbullying-detection-main/src/embeddings/__init__.py  \n",
            "  inflating: cyberbullying-detection-main/src/main.ipynb  \n",
            "  inflating: cyberbullying-detection-main/src/main.py  \n",
            "   creating: cyberbullying-detection-main/src/preprocessing/\n",
            "  inflating: cyberbullying-detection-main/src/preprocessing/TweetProcessor.py  \n",
            "  inflating: cyberbullying-detection-main/src/preprocessing/__init__.py  \n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/am-shb/cyberbullying-detection/archive/refs/heads/main.zip\n",
        "!unzip main.zip\n",
        "!mv cyberbullying-detection-main/src/* .\n",
        "!mv cyberbullying-detection-main/data/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1nHwvu0G5u6",
        "outputId": "71cbc39b-68e8-460a-f56d-aefcecf5cfb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 47692/47692 [09:30<00:00, 83.65it/s]\n",
            "100%|██████████| 47692/47692 [00:08<00:00, 5741.96it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from preprocessing import TweetProcessor\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "tp1 = TweetProcessor(lowercase=True, remove_stopwords=True, lemmatize=True,\n",
        "                    replace_urls='', replace_mentions='', replace_hashtags=False,\n",
        "                    convert_to_ascii=False, remove_punctuation=True,\n",
        "                    remove_numbers=True, replace_emojis=True,\n",
        "                    expand_contractions=True)\n",
        "\n",
        "\n",
        "tp2 = TweetProcessor(lowercase=True, remove_stopwords=True, lemmatize=False,\n",
        "                    replace_urls='', replace_mentions='', replace_hashtags=False,\n",
        "                    convert_to_ascii=False, remove_punctuation=True,\n",
        "                    remove_numbers=True, replace_emojis=True,\n",
        "                    expand_contractions=True)\n",
        "\n",
        "df = pd.read_csv('cyberbullying_tweets.csv')\n",
        "\n",
        "tweets1 = df['tweet_text'].progress_apply(tp1.transform).to_numpy()\n",
        "tweets2 = df['tweet_text'].progress_apply(tp2.transform).to_numpy()\n",
        "labels = df['cyberbullying_type'].to_numpy()\n",
        "binary_labels = ~(df['cyberbullying_type'] == 'not_cyberbullying').to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilTvDq_VHT0c"
      },
      "outputs": [],
      "source": [
        "from embeddings import FrequencyVectorizer, Word2vecVectorizer, SbertVectorizer\n",
        "embedders1 = {\n",
        "    'bow': FrequencyVectorizer(kind='bow', max_features=10000, ngram_range=(1, 5)),\n",
        "    'tfidf': FrequencyVectorizer(kind='tfidf', max_features=10000, ngram_range=(1, 5))\n",
        "}\n",
        "\n",
        "embedders2 = {\n",
        "    'w2v': Word2vecVectorizer('word2vec-google-news-300'),\n",
        "    'glove': Word2vecVectorizer('glove-wiki-gigaword-300'),\n",
        "    'ft': Word2vecVectorizer('fasttext-wiki-news-subwords-300'),\n",
        "    'sbert': SbertVectorizer('all-mpnet-base-v2')\n",
        "}\n",
        "\n",
        "embedders1['bow'].fit(tweets1)\n",
        "embedders1['tfidf'].fit(tweets1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYskukvhJM3h",
        "outputId": "e3131831-4470-4901-d747-d231aeed9cca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/embeddings/Word2vecVectorizer.py:22: RuntimeWarning: Mean of empty slice.\n",
            "  encoded = np.array([self.model[token] for token in tokenize(text) if token in self.model]).mean(axis=0)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ],
      "source": [
        "embeddings = {}\n",
        "for name, embedder in embedders1.items():\n",
        "    embeddings[name] = embedders1[name].batch_encode(tweets1)\n",
        "\n",
        "for name, embedder in embedders2.items():\n",
        "    embeddings[name] = embedders2[name].batch_encode(tweets2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y6tcP9MxJMq"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('embeddings.pkl', 'wb+') as f:\n",
        "  pickle.dump(embeddings, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cyberbullying.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
